# -*- coding: utf-8 -*-
"""Autoimmune_group.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H9aLh2b4-UwSVp4ckuEiVcvtFgjtI5t9

#***Data Definition***
A comprehensive set of variables related to demographics, health history, lab results, and reported symptoms to assess the presence of autoimmune conditions.

# ***Data Collection***
"""

# Import necessary libraries:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import warnings
warnings.filterwarnings('ignore')
df=pd.read_csv('/content/Autoimmune_Disorder_10k_with_All_Disorders after grouping.csv')
df

"""#***Data Exploration and Cleaning***

*a) Examine the data(understanding the structure and contents of the dataset)*
"""

# information on overall dataset
df.info()

# describe the numerical features in dataset
df.describe().transpose()

# Find the duplicate value
df.duplicated().sum()

df.isna().sum().sum()

df['Diagnosis'].to_string()

df.drop('Anti-dsDNA',axis=1,inplace=True)
# duplicate column of Anti_dsDNA

df['Anti-Sm'].unique()
df[df['Anti-Sm']==1][['Diagnosis']].value_counts()
# In a normal, healthy individual, Anti-Sm antibodies should be negative or undetectable.
df['Anti-Sm']=df['Anti-Sm'].fillna(0)

df['Rheumatoid factor'].unique()
df[df['Rheumatoid factor'].isna()][['Diagnosis']].value_counts()
# RF is low for healthy person
df['Rheumatoid factor']=df['Rheumatoid factor'].fillna(0)

df['ACPA'].unique()
df[df['ACPA'].isna()][['Diagnosis']].value_counts()
# In healthy individuals, ACPA(Anti-citrullinated protein antibodies) levels are typically very low or undetectable.
df['ACPA']=df['ACPA'].fillna(0)

df['Anti-TPO'].unique()
df[df['Anti-TPO'].isna()][['Diagnosis']].value_counts()
# Anti-TPO levels in healthy individuals will be low.
df['Anti-TPO']=df['Anti-TPO'].fillna(0)

df['Anti-Tg'].unique()
df[df['Anti-Tg'].isna()][['Diagnosis']].value_counts()
df['Anti-Tg']=df['Anti-Tg'].fillna(0)

df['Anti-SMA'].unique()
df[df['Anti-SMA'].isna()][['Diagnosis']].value_counts()
df['Anti-SMA']=df['Anti-SMA'].fillna(0)

df.isna().sum().sum()

df['Group'].value_counts()

df['Gender']=df['Gender'].map({'Male':1,'Female':0})
df

df.corr(numeric_only=True)

df.drop(['Patient_ID','Age','RBC_Count','Hemoglobin','Hematocrit','MCV','MCH','MCHC','Neutrophils','Lymphocytes','MPV','Anti_tTG','C3'],axis=1,inplace=True)
# df.drop(['Patient_ID','Age','RBC_Count','Hemoglobin','Hematocrit','MCV','MCH','MCHC','Neutrophils','Lymphocytes','MPV','Anti_tTG','C3','WBC_Count','PLT_Count','Anti_dsDNA','Anti_enterocyte_antibodies','anti_LKM1','Anti_RNP','ASCA','Anti_Ro_SSA','Anti_CBir1','Anti_BP230','Anti_tTG','DGP','Anti_BP180','ASMA','Anti_IF','IgG_IgE_receptor','Anti_SRP','Anti_desmoglein_3','Anti_La_SSB','Anti_Jo1','ANCA','anti_centromere','Anti_desmoglein_1','EMA','Anti_type_VII_collagen','C1_inhibitor','Anti_TIF1','Anti_epidermal_basement_membrane_IgA','Anti_OmpC','pANCA','Anti_tissue_transglutaminase','anti_Scl_70','Anti_Mi2','Anti_parietal_cell','Progesterone_antibodies','Anti_Sm','Anti-Sm'],axis=1,inplace=True)
df.columns

# Feature_Engineering
symptom_columns = ['Low-grade fever', 'Fatigue or chronic tiredness', 'Dizziness', 'Weight loss', 'Rashes and skin lesions', 'Stiffness in the joints', 'Brittle hair or hair loss', 'Dry eyes and/or mouth', 'General \'unwell\' feeling', 'Joint pain']
df['Symptom_Count'] = df[symptom_columns].sum(axis=1)
df.drop(symptom_columns, axis=1, inplace=True)

# # PCA
# from sklearn.decomposition import PCA

# # # Select the features to combine
# features_to_combine = ['CRP', 'ESR', 'MBL_Level', 'ANA', 'Sickness_Duration_Months']
# X_subset = df[features_to_combine]

# # # Apply PCA with 1 component
# pca = PCA(n_components=1)
# combined_feature = pca.fit_transform(X_subset)

# # Add the combined feature to your DataFrame
# df['Combined_Feature_PCA'] = combined_feature
# df.drop(features_to_combine, axis=1, inplace=True)

df.drop('Diagnosis',axis=1,inplace=True)

# df.drop(features_to_combine, axis=1, inplace=True)

x=df.drop('Group',axis=1)
y=df['Group']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)
y_train.shape

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
x_train, y_train = smote.fit_resample(x_train, y_train)
y_train.shape

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)
x_train.shape

from sklearn.preprocessing import PolynomialFeatures

     # Create a PolynomialFeatures object
poly = PolynomialFeatures(degree=2,include_bias=False)  # You can adjust the degree

# Apply polynomial transformation to your features
x_train = poly.fit_transform(x_train)
x_test = poly.transform(x_test)

x_train.shape

# Principal Component Analysis

from sklearn.decomposition import PCA
pca = PCA(n_components=5)
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)

x_train.shape

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
knn=KNeighborsClassifier()
svc=SVC()
rfc=RandomForestClassifier(class_weight='balanced')
dtc=DecisionTreeClassifier()
gnb=BernoulliNB()
lst=[knn,svc,rfc,dtc,gnb]
for i in lst:
  print(f'Model is {i}')
  print("*************")
  i.fit(x_train,y_train)
  y_pred=i.predict(x_test)
  print(f'Accuracy is {accuracy_score(y_test,y_pred)}')
  print(f'classification report is\n {classification_report(y_test,y_pred)}')

# ... (your existing code for data loading, preprocessing, feature engineering)

# Logistic Regression Model
from sklearn.linear_model import LogisticRegression
logreg_model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)
logreg_model.fit(x_train, y_train)

# Predictions and Evaluation
y_pred = logreg_model.predict(x_test)
# ... (your existing code for model evaluation using accuracy, classification report, etc.)
print(f'Accuracy is {accuracy_score(y_test,y_pred)}')
print(f'classification report is {classification_report(y_test,y_pred)}')

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Define your base models (estimators)
estimators = [
    ('rfc', RandomForestClassifier()),
    ('knn', KNeighborsClassifier()),
    ('bnb', BernoulliNB())
]

# Define the meta-learner (final estimator)
stacking_model = StackingClassifier(
    estimators=estimators, final_estimator=LogisticRegression()
)

# Train the stacking model
stacking_model.fit(x_train, y_train)

# Make predictions using the stacking model
y_pred = stacking_model.predict(x_test)

# Evaluate the stacking model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# # Random Search CV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the hyperparameter grid:

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

# Number of features to consider at every split
max_features = ['auto', 'sqrt']

# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)

# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]

# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]

# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(random_grid)

# # Create and train the RandomizedSearchCV object:

# # Use the random grid to search for best hyperparameters
# # First create the base model to tune
# rf = RandomForestClassifier()

# # Random search of parameters, using 3 fold cross validation,
# # search across 100 different combinations, and use all available cores
# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)

# # Fit the random search model
# rf_random.fit(x_train, y_train)

# # Get the best parameters:
# rf_random.best_params_

# # Evaluate the model with the best parameters:
# best_random = rf_random.best_estimator_
# y_pred_random = best_random.predict(x_test)
# print(f'Accuracy is {accuracy_score(y_test,y_pred_random)}')
# print(f'classification report is {classification_report(y_test,y_pred_random)}')